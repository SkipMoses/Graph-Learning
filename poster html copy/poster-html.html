<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">


<!--
Font-awesome icons ie github or twitter
-->
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/brands.css" integrity="sha384-n9+6/aSqa9lBidZMRCQHTHKJscPq6NW4pCQBiMmHdUCvPN8ZOg2zJJTkC7WIezWv" crossorigin="anonymous">

<!--
Google fonts api stuff
-->



<title>Graph Topology Learning from Binary Signals on Graph</title>

<script src="poster-html_files/header-attrs-2.10/header-attrs.js"></script>





<style>
@page {
size: 60in 36in;
margin: 0;
padding: 0;
}
body {
margin: 0;
font-size: 45px;
width: 60in;
height: 36in;
padding: 0;
text-align: justify;
font-family: Palatino;
}
.poster_wrap {
width: 60in;
height: 36in;
padding: 0cm;
}
.title_container {
width: 60in;
height: calc(36in * 0.15);
overflow: hidden;
background-color: #9D2235;
border: 0 solid #FFFFFF;
}
.logo_left {
float: left;
width: 10%;
height: 100%;
background-color: #9D2235;
display: flex;
align-items: center;
justify-content: center;
}
.logo_right {
float: right;
width: 10%;
height: 100%;
background-color: #9D2235;
display: flex;
align-items: center;
justify-content: center;
}
.poster_title {
text-align: center;
position: relative;
float: left;
width: 80%;
height: 100%;
color: #FFFFFF;
top: 50%;
transform: translateY(-50%);
-webkit-transform: translateY(-50%);
}
#title {
font-family: Palatino;
}
/* unvisited link */
a:link {
color: #cc0000;
}
.mybreak {
  break-before: column;
}
/* visited link */
a:visited {
color: #cc0000;
}

/* mouse over link */
a:hover {
color: #cc0000;
}

/* selected link */
a:active {
color: #cc0000;
}
.poster_body {
-webkit-column-count: 4; /* Chrome, Safari, Opera */
-moz-column-count: 4; /* Firefox */
column-count: 4;
-webkit-column-fill: auto;
-moz-column-fill: auto;
column-fill: auto;
-webkit-column-rule-width: 1mm;
-moz-column-rule-width: 1mm;
column-rule-width: 1mm;
-webkit-column-rule-style: dashed;
-moz-column-rule-style: dashed;
column-rule-style: dashed;
-webkit-column-rule-color: #9D2235;
-moz-column-rule-color: #9D2235;
column-rule-color: #9D2235;
column-gap: 1em;
padding-left: 0.5em;
padding-right: 0.5em;
height: 100%;
color: #000000
background-color: #ffffff;
}
.poster_title h1 {
font-size: 75pt;
margin: 0;
border: 0;
font-weight: normal;
}
.poster_body_wrap{
width: calc(60in + 0 + 0);
height: calc(36in * 0.83);
padding-top: calc(36in * 0.005);
padding-bottom: calc(36in * 0.01);
background-color: #ffffff;
}
.poster_title h3 {
color: #FFFFFF;
font-size: 50pt;
margin: 0;
border: 0;
font-weight: normal;
}
.poster_title h3 > sup {
  font-size: 35pt;
  margin-left: 0.02em;
}
.poster_title h5 {
color: #FFFFFF;
font-size: 35pt;
margin: 0;
border: 0;
font-weight: normal;
}
img {
margin-top: 2cm;
margin-bottom: 0;
}
.section {
  padding: 0.2em;
}
.poster_body h1 {
text-align: center;
color: #FFFFFF;
font-size: 65pt;
border: 2mm solid #9D2235;
background-color: #9D2235;
border-radius: 4mm 0mm;
margin-top: 2mm;
margin-bottom: 2mm;
font-weight: normal;
}
.poster_body h2 {
color: #000000;
font-size: 40pt;
padding-left: 4mm;
font-weight: normal;
}
.span {
width: 200%;
}
/* center align leaflet map,
from https://stackoverflow.com/questions/52112119/center-leaflet-in-a-rmarkdown-document */
.html-widget {
margin: auto;
position: sticky;
margin-top: 2cm;
margin-bottom: 2cm;
}
.leaflet.html-widget.html-widget-static-bound.leaflet-container.leaflet-touch.leaflet-fade-anim.leaflet-grab.leaflet-touch-drag.leaflet-touch-zoom {
position: sticky;
width: 100%;
}
pre.sourceCode.r {
background-color: #dddddd40;
border-radius: 4mm;
padding: 4mm;
width: 75%;
/* align-items: center; */
margin: auto;
padding-left: 2cm;
}
code.sourceCode.r{
background-color: transparent;
font-size: 20pt;
border-radius: 2mm;
}
.caption {
font-size: 25pt;
}
.table caption {
font-size: 25pt;
padding-bottom: 3mm;

}
code {
font-size: 1em;
font-family: monospace;
background-color: #FFFFFF24;
color: #9D2235;
padding: 1.2mm;
border-radius: 2mm;
}
.poster_title code {
font-size: 1em;
}
table {
font-size: 40px;
margin: auto;
border-top: 3px solid #666;
border-bottom: 3px solid #666;
}
table thead th {
border-bottom: 3px solid #ddd;
}
td {
padding: 8px;
}
th {
padding: 15px;
}
caption {
margin-bottom: 10px;
}
.poster_body p {
margin-right: 4mm;
margin-left: 4mm;
margin-top: 6mm;
margin-bottom: 10mm;
}
.poster_body ol {
margin-right: 4mm;
margin-left: 4mm;
}
#ul {
margin-right: 4mm;
margin-left: 4mm;
}
.references p {
font-size: 20pt;
}
.orcid img {
  width: 1em;
}
</style>
</head>
<body>


<div class="poster_wrap">
<div class="title_container">
<!-- Left Logo  -->
<div class="logo_left">
</div>
<!-- Poster Title -->
<div class= "poster_title">
<h1 id="title">Graph Topology Learning from Binary Signals on Graph</h1>
<h3 id="author">Skip Moses<sup></sup>, Jing Guo<sup></sup></h3><br>
<h5 id="affiliation"><sup></sup> Department of Mathematics and Statistics, California State University, Chico</h5>
</div>
<!-- Right Logo  -->
<div class="logo_right">
<img src=Chico-State-Flame-Cardinal300dpi.png style="width: 80%">
</div>
</div>

<div class='poster_body_wrap'>
<div class='poster_body'>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Data often has an underlying structure or geometry that can be modeled as a signal on the vertices of a weighted, undirected graph. Traditionally, the emphasis was on using a underlying graph, or network, to understand the properties of signals over the vertices. Recently, there has been a surge in converse problem; learning a graph structure from a set of signals satisfying some constraints <span class="citation">(Dong et al. 2016)</span> <span class="citation">(Ortega et al. 2018)</span>. In previous research <span class="citation">(Dong et al. 2016)</span>, <span class="citation">(Tugnait 2021)</span>, and <span class="citation">(Saboksayr and Mateos 2021)</span>, signals on graph were assumed to follow multivariate Gaussian distributions, but there has been little exploration in learning a network from binomial signals. In this work, we develop a novel methodology that allows for learning graph topology given a set of binary signals on the graph as shown in Figure <a href="#fig:GTG">1</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:GTG"></span>
<img src="pictures/gt_graph.png" alt="Circular Embedding of random Erd&amp;#337;s-Reyni Graph with edge probability p = 0.2. Each edge weight follows uniform random distribution on (0,1). Grey nodes indicate a signal value of 0 and black nodes indicate a signal value of 1." width="90%" height="25%" />
<p class="caption">
Figure 1: Circular Embedding of random Erdős-Reyni Graph with edge probability p = 0.2. Each edge weight follows uniform random distribution on (0,1). Grey nodes indicate a signal value of 0 and black nodes indicate a signal value of 1.
</p>
</div>
</div>
<div id="methods" class="section level1">
<h1>Methods</h1>
<div id="signals-on-the-graph-and-model-specification" class="section level4">
<h4>Signals on the graph and model specification</h4>
<p>We consider a weighted undirected graph <span class="math inline">\(G = (V, E)\)</span>, with the vertices set <span class="math inline">\(V = {1, 2, \dots, N}\)</span>, and edge set <span class="math inline">\(E\)</span>. Let <span class="math inline">\(\mathbf{A}\)</span> denote the weighted adjacency matrix of <span class="math inline">\(G\)</span>. In the case of weighted undirected graph, <span class="math inline">\(\mathbf{A}\)</span> is a square and symmetric matrix.</p>
<p>Let <span class="math inline">\(\mathbf{Y}_{i,j}\)</span> denote the signal on the node <span class="math inline">\(i\)</span> of graph <span class="math inline">\(G\)</span> at round <span class="math inline">\(j\)</span>, where <span class="math inline">\(j = 1, \dots, M\)</span>, and <span class="math inline">\(i = 1, \dots, N\)</span>. We assume that <span class="math inline">\(\mathbf{Y}_{i,j}\)</span> is a binary signal that can be 1, or 0.
Suppose the signals at stratum <span class="math inline">\(j\)</span> denoted by <span class="math inline">\(\mathbf{Y}[, j]\)</span> for all <span class="math inline">\(N\)</span> nodes are independent of the signals at stratum <span class="math inline">\(k\)</span> denoted by <span class="math inline">\(\mathbf{Y}[,k ]\)</span>, for <span class="math inline">\(j \neq k\)</span>. Let <span class="math inline">\(p_{i,j}\)</span> denote the probability of <span class="math inline">\(\mathbf{Y}_{i,j} = 1\)</span>. Our model assumes</p>
<p><span class="math display">\[\begin{equation}
\label{eq: binaryglm}
\text{logit}(p_{i,j}) = (\mathbf{A} h )_i,
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mathbf{A}\)</span> is the adjacency matrix from the graph <span class="math inline">\(G\)</span>, <span class="math inline">\(h\)</span> is a vector of latent factors that governs <span class="math inline">\(p_{i, j}\)</span> through <span class="math inline">\(\mathbf{A}\)</span> and assumed to be a standard normal random vector.</p>
<div id="maximum-likelihood-for-one-stratum" class="section level5">
<h5>Maximum likelihood for one stratum</h5>
<p>Consider the probability mass function for a given <span class="math inline">\(\mathbf{A}\)</span> and signal <span class="math inline">\(y = \mathbf{Y}[,k]\)</span>
<span class="math display">\[\begin{align}
P_{\mathbf{A}h}(y_i) &amp;= p^{y_i}(1-p)^{1-y_i} \\
            &amp;= \left(\frac{e^{\mathbf{A}[i,]h}}{1 + e^{\mathbf{A}[i,]h}}\right)^{y_i}\left(1- \frac{e^{\mathbf{A}[i,]h}}{1 + e^{\mathbf{A}[i,]h}}\right)^{1-y_i} \\
            &amp;= \left(\frac{e^{\mathbf{A}[i,]h}}{1 + e^{\mathbf{A}[i,]h}}\right)^{y_i}\left(\frac{1}{1 + e^{\mathbf{A}[i,]h}}\right)^{1-y_i}   \\
            &amp;= \frac{e^{y_i\mathbf{A}[i,]h}}{1 + e^{\mathbf{A}[i,]h}}
            
\end{align}\]</span></p>
<p>Therefore, our Likelihood function will be given by</p>
<p><span class="math display">\[\begin{align}
\mathcal{L}(h) = \prod_{i=1}^N\left(\frac{e^{y_i\mathbf{A}[i,]h}}{1 + e^{\mathbf{A}[i,]h}}\right)
\end{align}\]</span></p>
<p>In order to maximize we consider the natural logarithm of our likelihood</p>
<p><span class="math display">\[\begin{align}
\log(\mathcal{L}(h)) = \sum_{i=1}^N\left(y_i(\mathbf{A}[i,]h) - \log(1 + e^{\mathbf{A}[i,]h})\right)
\end{align}\]</span></p>
</div>
<div id="optimization-program" class="section level5">
<h5>Optimization Program</h5>
<p>Taking inspiration from the above derivation we will solve for estimated <span class="math inline">\(\mathbf{A}\)</span> by maximizing the following</p>
<p><span class="math display">\[\begin{equation}
\label{eq:optimization}
\begin{aligned}
&amp;\max_{\mathbf{A},h} \sum_{j = 1}^M\sum_{i=1}^N\left(y_{i,j}(\mathbf{A}[i,]h) - \log(1 + e^{\mathbf{A}[i,]h})\right) - \alpha \vert L \vert _ F \\
\textrm{s.t.} \quad  &amp;\mathbf{A}_{i,j} = 0 \text{ if } i =j;\,\, \mathbf{A}_{i,j} \geq 0 \text{ if } i \neq j\\
                     &amp;\max(h) \leq a ;  \,\, \min(h) \geq b \,\, ; \mathbb{1}h^T = 0  
\end{aligned}
\end{equation}\]</span>
where <span class="math inline">\(\alpha\)</span> is a tuning parameter for controlling the sparsity and <span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(b\)</span> are tuning parameters for restricting the spread of the values of <span class="math inline">\(h\)</span>.</p>
<p>We presented our optimization algorithm below. In each iteration of step 5) and 6) of Algorithm 1, the optimization program gives a Disciplined Concave Program that can be solved efficiently in Python with the CVXPY library.</p>
<hr />
<span class="math display">\[\begin{align*}
&amp;\textbf{Algorithm 1: Binary Signal Graph Learning} \\
&amp;1) \textbf{ Input: } \text{ Input a signal } \mathbf{Y}.\\
&amp;2) \textbf{ Output: } \text{ Output an estimated } \mathbf{A}.\\
&amp;3) \textbf{ Initialization: } h_{i,0} \sim \mathcal{N}(0,1) \text{ for } i = 1,\ldots, N \\
&amp;4) \textbf{ for } t = 1, \ldots, iter:\\
&amp;5) \textbf{ Update } \mathbf{A} \textbf{ given } h:\\
&amp;6) \,\,\,\, \textit{ Fix }  h \textit{ in Optimization Program and solve } \mathbf{A} \\
&amp;7) \textbf{        Update } h \textbf{ given } \mathbf{A}:\\
&amp;8) \,\,\,\, \textit{ Fix } \mathbf{A} \text{ in Optimization Program and solve h} \\
&amp;9) \textbf{ end for }
\end{align*}\]</span>
<hr />
</div>
</div>
</div>
<div id="results" class="section level1">
<h1>Results</h1>
<div id="synthetic-data-generation" class="section level5">
<h5>Synthetic Data Generation</h5>
<p>Let <span class="math inline">\(\mathbf{A}\)</span> be the adjacency matrix of the graph in Figure <a href="#fig:GTG">1</a>, and <span class="math inline">\(h \in \mathbb{R}\)</span> be such that <span class="math inline">\(h_i \sim \mathcal{N}(0,1)\)</span>. Set <span class="math inline">\(p\)</span> as a logistic function of <span class="math inline">\(\mathbf{A}h\)</span>, and compare <span class="math inline">\(p\)</span> to a vector <span class="math inline">\(t\)</span>, where each <span class="math inline">\(t_i\)</span> follows a random uniform distribution <span class="math inline">\(\mathcal{U}(0,1)\)</span>. If <span class="math inline">\(t_k &lt; p_k\)</span>, then node <span class="math inline">\(k\)</span> takes the signal value of <span class="math inline">\(1\)</span>, and <span class="math inline">\(0\)</span> otherwise. We generate <span class="math inline">\(M = 100\)</span> synthetic signals, and use them to learn an adjacency matrix.</p>
</div>
<div id="graph-learning-results" class="section level5">
<h5>Graph Learning Results</h5>
<p>The learned graph is found with parameter values <span class="math inline">\(\alpha = .2\)</span>, <span class="math inline">\(a = 1\)</span> and <span class="math inline">\(b = -1\)</span> by implementing Algorithm 1. We can see the learned adjacency matrix shares some similar features to the ground truth (Figure <a href="#fig:HM">2</a>), but the learned adjacency matrix is sparser than the ground truth (Figure <a href="#fig:HM">2</a> and Figure <a href="#fig:EAG">3</a>).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:HM"></span>
<img src="pictures/gt_A_heatmap.png" alt="(Left) A heatmap of the ground truth adjacency matrix. (Right) A heatmap of the estimated adjacency matrix. " width="49%" height="20%" /><img src="pictures/est_A_heatmap.png" alt="(Left) A heatmap of the ground truth adjacency matrix. (Right) A heatmap of the estimated adjacency matrix. " width="49%" height="20%" />
<p class="caption">
Figure 2: (Left) A heatmap of the ground truth adjacency matrix. (Right) A heatmap of the estimated adjacency matrix.
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:EAG"></span>
<img src="pictures/est_graph.png" alt="Circular Embedding of Estimated graph. Note the overall weight of each edge is smaller than the edge weights of the ground truth." width="60%" height="10%" />
<p class="caption">
Figure 3: Circular Embedding of Estimated graph. Note the overall weight of each edge is smaller than the edge weights of the ground truth.
</p>
</div>
</div>
</div>
<div id="discussion" class="section level1">
<h1>Discussion</h1>
<p>To our knowledge, this is the first work on learning graph topology from binary signals. We have developed a method to effectively learn the graph adjacency matrix from binary signals. This model can be easily extended for learning binomial signals in general. Incorporating parameter optimization techniques would be a natural improvement on this model. Moreover, it would be desirable to develop scalable algorithms that can be used to learn a topology for a large graph from signals. Recently, in <span class="citation">Saboksayr and Mateos (2021)</span> the author shows fast proximal-gradient iterations can be applied to the framework given by <span class="citation">Kalofolias (2016)</span> improving the over-all runtime.</p>
</div>
<div id="acknowledgement" class="section level1">
<h1>Acknowledgement</h1>
<p>This work is supported by Research, Scholarship, and Creative Activity (RSCA) Award 2021-2022 through Chico State Enterprises. Thank you to Robin Donatello for approving this work for Data Science Capstone Project.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-dong2016learning" class="csl-entry">
Dong, Xiaowen, Dorina Thanou, Pascal Frossard, and Pierre Vandergheynst. 2016. <span>“Learning Laplacian Matrix in Smooth Graph Signal Representations.”</span> <em>IEEE Transactions on Signal Processing</em> 64 (23): 6160–73.
</div>
<div id="ref-kalofolias2016learn" class="csl-entry">
Kalofolias, Vassilis. 2016. <span>“How to Learn a Graph from Smooth Signals.”</span> In <em>Artificial Intelligence and Statistics</em>, 920–29. PMLR.
</div>
<div id="ref-ortega2018graph" class="csl-entry">
Ortega, Antonio, Pascal Frossard, Jelena Kovačević, José MF Moura, and Pierre Vandergheynst. 2018. <span>“Graph Signal Processing: Overview, Challenges, and Applications.”</span> <em>Proceedings of the IEEE</em> 106 (5): 808–28.
</div>
<div id="ref-saboksayr2021accelerated" class="csl-entry">
Saboksayr, Seyed Saman, and Gonzalo Mateos. 2021. <span>“Accelerated Graph Learning from Smooth Signals.”</span> <em>IEEE Signal Processing Letters</em> 28: 2192–96.
</div>
<div id="ref-tugnait2021sparse" class="csl-entry">
Tugnait, Jitendra K. 2021. <span>“Sparse Graph Learning Under Laplacian-Related Constraints.”</span> <em>IEEE Access</em> 9: 151067–79.
</div>
</div>
</div>
</div>
</div>

</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
var script = document.createElement("script");
script.type = "text/javascript";
var src = "true";
if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
if (location.protocol !== "file:" && /^https?:/.test(src))
src = src.replace(/^https?:/, '');
script.src = src;
document.getElementsByTagName("head")[0].appendChild(script);
})();
</script>


</body>
</html>
