<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">


<!--
Font-awesome icons ie github or twitter
-->
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/brands.css" integrity="sha384-n9+6/aSqa9lBidZMRCQHTHKJscPq6NW4pCQBiMmHdUCvPN8ZOg2zJJTkC7WIezWv" crossorigin="anonymous">

<!--
Google fonts api stuff
-->



<title>Graph Topology Learning from Binary Signals on Graph</title>

<script src="poster-html_files/header-attrs-2.11/header-attrs.js"></script>

<script src="poster-html_files/accessible-code-block-0.0.1/empty-anchor.js"></script>




<style>
@page {
size: 60in 36in;
margin: 0;
padding: 0;
}
body {
margin: 0;
font-size: 45px;
width: 60in;
height: 36in;
padding: 0;
text-align: justify;
font-family: Palatino;
}
.poster_wrap {
width: 60in;
height: 36in;
padding: 0cm;
}
.title_container {
width: 60in;
height: calc(36in * 0.15);
overflow: hidden;
background-color: #9D2235;
border: 0 solid #FFFFFF;
}
.logo_left {
float: left;
width: 10%;
height: 100%;
background-color: #9D2235;
display: flex;
align-items: center;
justify-content: center;
}
.logo_right {
float: right;
width: 10%;
height: 100%;
background-color: #9D2235;
display: flex;
align-items: center;
justify-content: center;
}
.poster_title {
text-align: center;
position: relative;
float: left;
width: 80%;
height: 100%;
color: #FFFFFF;
top: 50%;
transform: translateY(-50%);
-webkit-transform: translateY(-50%);
}
#title {
font-family: Palatino;
}
/* unvisited link */
a:link {
color: #cc0000;
}
.mybreak {
  break-before: column;
}
/* visited link */
a:visited {
color: #cc0000;
}

/* mouse over link */
a:hover {
color: #cc0000;
}

/* selected link */
a:active {
color: #cc0000;
}
.poster_body {
-webkit-column-count: 4; /* Chrome, Safari, Opera */
-moz-column-count: 4; /* Firefox */
column-count: 4;
-webkit-column-fill: auto;
-moz-column-fill: auto;
column-fill: auto;
-webkit-column-rule-width: 1mm;
-moz-column-rule-width: 1mm;
column-rule-width: 1mm;
-webkit-column-rule-style: dashed;
-moz-column-rule-style: dashed;
column-rule-style: dashed;
-webkit-column-rule-color: #9D2235;
-moz-column-rule-color: #9D2235;
column-rule-color: #9D2235;
column-gap: 1em;
padding-left: 0.5em;
padding-right: 0.5em;
height: 100%;
color: #000000
background-color: #ffffff;
}
.poster_title h1 {
font-size: 75pt;
margin: 0;
border: 0;
font-weight: normal;
}
.poster_body_wrap{
width: calc(60in + 0 + 0);
height: calc(36in * 0.83);
padding-top: calc(36in * 0.005);
padding-bottom: calc(36in * 0.01);
background-color: #ffffff;
}
.poster_title h3 {
color: #FFFFFF;
font-size: 50pt;
margin: 0;
border: 0;
font-weight: normal;
}
.poster_title h3 > sup {
  font-size: 35pt;
  margin-left: 0.02em;
}
.poster_title h5 {
color: #FFFFFF;
font-size: 35pt;
margin: 0;
border: 0;
font-weight: normal;
}
img {
margin-top: 2cm;
margin-bottom: 0;
}
.section {
  padding: 0.2em;
}
.poster_body h1 {
text-align: center;
color: #FFFFFF;
font-size: 65pt;
border: 2mm solid #9D2235;
background-color: #9D2235;
border-radius: 4mm 0mm;
margin-top: 2mm;
margin-bottom: 2mm;
font-weight: normal;
}
.poster_body h2 {
color: #000000;
font-size: 40pt;
padding-left: 4mm;
font-weight: normal;
}
.span {
width: 200%;
}
/* center align leaflet map,
from https://stackoverflow.com/questions/52112119/center-leaflet-in-a-rmarkdown-document */
.html-widget {
margin: auto;
position: sticky;
margin-top: 2cm;
margin-bottom: 2cm;
}
.leaflet.html-widget.html-widget-static-bound.leaflet-container.leaflet-touch.leaflet-fade-anim.leaflet-grab.leaflet-touch-drag.leaflet-touch-zoom {
position: sticky;
width: 100%;
}
pre.sourceCode.r {
background-color: #dddddd40;
border-radius: 4mm;
padding: 4mm;
width: 75%;
/* align-items: center; */
margin: auto;
padding-left: 2cm;
}
code.sourceCode.r{
background-color: transparent;
font-size: 20pt;
border-radius: 2mm;
}
.caption {
font-size: 25pt;
}
.table caption {
font-size: 25pt;
padding-bottom: 3mm;

}
code {
font-size: 1em;
font-family: monospace;
background-color: #FFFFFF24;
color: #9D2235;
padding: 1.2mm;
border-radius: 2mm;
}
.poster_title code {
font-size: 1em;
}
table {
font-size: 40px;
margin: auto;
border-top: 3px solid #666;
border-bottom: 3px solid #666;
}
table thead th {
border-bottom: 3px solid #ddd;
}
td {
padding: 8px;
}
th {
padding: 15px;
}
caption {
margin-bottom: 10px;
}
.poster_body p {
margin-right: 4mm;
margin-left: 4mm;
margin-top: 6mm;
margin-bottom: 10mm;
}
.poster_body ol {
margin-right: 4mm;
margin-left: 4mm;
}
#ul {
margin-right: 4mm;
margin-left: 4mm;
}
.references p {
font-size: 20pt;
}
.orcid img {
  width: 1em;
}
</style>
</head>
<body>


<div class="poster_wrap">
<div class="title_container">
<!-- Left Logo  -->
<div class="logo_left">
</div>
<!-- Poster Title -->
<div class= "poster_title">
<h1 id="title">Graph Topology Learning from Binary Signals on Graph</h1>
<h3 id="author">Skip Moses<sup></sup></h3><br>
<h5 id="affiliation"><sup></sup> Department of Mathematics and Statistics, California State University, Chico</h5>
</div>
<!-- Right Logo  -->
<div class="logo_right">
</div>
</div>

<div class='poster_body_wrap'>
<div class='poster_body'>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Data often has an underlying structure or geometry that can be modeled as a signal on the vertices of a weighted, undirected graph. Traditionally, the emphasis was on using a underlying graph, or network, to understand the properties of signals over the vertices. Recently, there has been a surge in converse problem; of learning a graph structure from a set of signals satisfying some constraints <span class="citation">(Xia et al. 2021)</span> <span class="citation">(Dong et al. 2016)</span> <span class="citation">(Ortega et al. 2018)</span>. In previous research <span class="citation">(Dong et al. 2016)</span>, <span class="citation">(Tugnait 2021)</span>, <span class="citation">(Pu et al. 2021)</span>, and <span class="citation">(Saboksayr and Mateos 2021)</span>, signals on graph were assumed to follow multivariate Gaussian distributions, but there has been little exploration in learning a network from binomial signals. In this work, we develop a novel methodology that allows for learning graph topology given a set of binary signals on the graph. An example of such signals on graph is presented in Figure 1.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:GTG"></span>
<img src="pictures/gt_graph.png" alt="Circular Embedding of ground truth graph" width="90%" height="20%" />
<p class="caption">
Figure 1: Circular Embedding of ground truth graph
</p>
</div>
</div>
<div id="methods" class="section level1">
<h1>Methods</h1>
<div id="signals-on-the-graph-and-model-specification" class="section level4">
<h4>Signals on the graph and model specification</h4>
<p>We consider a weighted undirected graph <span class="math inline">\(G = (V, E)\)</span>, with the vertices set <span class="math inline">\(V = {1, 2, \dots, N}\)</span>, and edge set <span class="math inline">\(E\)</span>. Let <span class="math inline">\(\mathbf{A}\)</span> denote the weighted adjacency matrix for the graph <span class="math inline">\(G\)</span>. In the case of weighted undirected graph, <span class="math inline">\(\mathbf{A}\)</span> is a square and symmetric matrix.</p>
<p>Let <span class="math inline">\(Y_{i,j}\)</span> denote the signal on the node <span class="math inline">\(i\)</span> of graph <span class="math inline">\(G\)</span> at round <span class="math inline">\(j\)</span>, where <span class="math inline">\(j = 1, \dots, M\)</span>, and <span class="math inline">\(i = 1, \dots, N\)</span>. We assume that <span class="math inline">\(Y_{i,j}\)</span> is a binary signal that can be 1, or 0.
Suppose the signals at stratum <span class="math inline">\(j\)</span> denoted by <span class="math inline">\(Y[, j]\)</span> for all <span class="math inline">\(N\)</span> nodes are independent of the signals at stratum <span class="math inline">\(k\)</span> denoted by <span class="math inline">\(Y[,k ]\)</span>, for <span class="math inline">\(j \neq k\)</span>. Let <span class="math inline">\(p_{i,j}\)</span> denote the probability of <span class="math inline">\(Y_{i,j} = 1\)</span>. Our model assumes</p>
<p><span class="math display">\[\begin{equation}
\label{eq: binaryglm}
\text{logit}(p_{i,j}) = (\mathbf{A} h )_i,
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mathbf{A}\)</span> is the adjacency matrix from the graph <span class="math inline">\(G\)</span>, <span class="math inline">\(h\)</span> is a vector of latent factors that governs <span class="math inline">\(p_{i, j}\)</span> through <span class="math inline">\(\mathbf{A}\)</span> and assumed to be a standard normal random vector.</p>
</div>
<div id="method-of-estimation" class="section level4">
<h4>Method of Estimation</h4>
<div id="maximum-likelihood-for-one-stratum" class="section level5">
<h5>Maximum likelihood for one stratum</h5>
<p>Consider the probability mass function for a given <span class="math inline">\(A\)</span> and signal <span class="math inline">\(y = Y[,k]\)</span>
<span class="math display">\[\begin{align}
P_{Ah}(y_i) &amp;= p^{y_i}(1-p)^{1-y_i} \\
            &amp;= \left(\frac{e^{A[i,]h}}{1 + e^{A[i,]h}}\right)^{y_i}\left(1- \frac{e^{A[i,]h}}{1 + e^{A[i,]h}}\right)^{1-y_i} \\
            &amp;= \left(\frac{e^{A[i,]h}}{1 + e^{A[i,]h}}\right)^{y_i}\left(\frac{1}{1 + e^{A[i,]h}}\right)^{1-y_i}   \\
            &amp;= \frac{e^{y_iA[i,]h}}{1 + e^{A[i,]h}}
            
\end{align}\]</span></p>
<p>Therefore, our Likelihood function will be given by</p>
<p><span class="math display">\[\begin{align}
\mathcal{L}(h) = \prod_{i=1}^N\left(\frac{e^{y_iA[i,]h}}{1 + e^{A[i,]h}}\right)
\end{align}\]</span></p>
<p>In order to maximize we consider the natural logarithm of our likelihood</p>
<p><span class="math display">\[\begin{align}
\log(\mathcal{L}(h)) = \sum_{i=1}^N\left(y_i(A[i,]h) - \log(1 + e^{A[i,]h})\right)
\end{align}\]</span></p>
</div>
<div id="optimization-program" class="section level5">
<h5>Optimization Program</h5>
<p>Taking inspiration from the above derivation we will solve for estimated <span class="math inline">\(A\)</span> by maximizing the following</p>
<p><span class="math display">\[\begin{equation}
\label{eq:optimization}
\begin{aligned}
&amp;\max_{A,h} \sum_{j = 1}^M\sum_{i=1}^N\left(y_{i,j}(A[i,]h) - \log(1 + e^{A[i,]h})\right) - \alpha \vert L \vert _ F \\
\textrm{s.t.} \quad  &amp;A_{i,j} = 0 \text{ if } i =j\\
                     &amp;A_{i,j} \geq 0 \text{ if } i \neq j\\
                     &amp;\mathbb{1}h^T = 0 \\
                     &amp;\max(h) \leq a \\
                     &amp;\min(h) \geq b
\end{aligned}
\end{equation}\]</span>
where <span class="math inline">\(\alpha\)</span> is a tuning parameter for controlling the sparsity and <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are tuning parameters for restricting the spread of the values of <span class="math inline">\(h\)</span>.</p>
<hr />
<hr />
<span class="math inline">\(\textbf{Algorithm 1:}\)</span> Estimate <span class="math inline">\(A\)</span> given <span class="math inline">\(Y\)</span>.
<hr />
<hr />
<span class="math display">\[\begin{align*}
&amp;1) \textbf{ Input: } \text{ Input a signal } Y.\\
&amp;2) \textbf{ Output: } \text{ Output an estimated } A.\\
&amp;3) \textbf{ Initialization: } h_{i,0} \sim \mathcal{N}(0,1) \text{ for } i = 1,\ldots, N \\
&amp;4) \textbf{ for } t = 1, \ldots, iter:\\
&amp;5) \textbf{ Update } A \textbf{ given } h:\\
&amp;6) \textbf{        Update } h \textbf{ given } A:\\
&amp;7) \textbf{ end for }
\end{align*}\]</span>
<hr />
</div>
</div>
</div>
<div id="results" class="section level1">
<h1>Results</h1>
<p>In each iteration of step 5) and 6) of Algorithm 1, the optimization program gives a Disciplined Concave Program that can be solved efficiently in Python with the CVXPY library. We use a random Erdős-Reyni graph on <span class="math inline">\(N = 20\)</span> nodes and edge probability <span class="math inline">\(p = 0.2\)</span>. Each edge weight <span class="math inline">\(w_i \sim \mathcal{U}(0,1)\)</span>.</p>
<div id="synthetic-data-generation" class="section level5">
<h5>Synthetic Data Generation</h5>
<p>To generate a single signal for a given adjacency matrix <span class="math inline">\(A \in \mathbb{R}^{N\times N}\)</span> and latent factor vector <span class="math inline">\(h \in \mathbb{R}^N\)</span> (each <span class="math inline">\(h_i \sim \mathcal{N}(0,1)\)</span>), a vector of odds, <span class="math inline">\(Ah\)</span>, is compared to a vector <span class="math inline">\(T\)</span>, where <span class="math inline">\(T_i\)</span> selected uniformly at random for each <span class="math inline">\(i = 1, \ldots , N\)</span>. If <span class="math inline">\(T_k &lt; Ah_k\)</span>, then node <span class="math inline">\(k\)</span> takes the signal value of <span class="math inline">\(1\)</span>, and <span class="math inline">\(0\)</span> otherwise. We generate <span class="math inline">\(M = 100\)</span> synthetic signals, and use them to learn an adjacency matrix. The learned graph is found with parameter values <span class="math inline">\(\alpha = .2\)</span>, <span class="math inline">\(a = 1\)</span> and <span class="math inline">\(b = -1\)</span>.</p>
<p>Heat maps of the ground truth adjacency matrix and estimated adjacency matrix are given in Figure 2, along with circular embedding of the ground truth and estimated graphs in Figure 3.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:HM"></span>
<img src="pictures/gt_A_heatmap.png" alt="(Left) A heatmap of the ground truth adjacency matrix. (Right) A heatmap of the estimated adjacency matrix." width="49%" height="20%" /><img src="pictures/est_A_heatmap.png" alt="(Left) A heatmap of the ground truth adjacency matrix. (Right) A heatmap of the estimated adjacency matrix." width="49%" height="20%" />
<p class="caption">
Figure 2: (Left) A heatmap of the ground truth adjacency matrix. (Right) A heatmap of the estimated adjacency matrix.
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:EAG"></span>
<img src="pictures/est_graph.png" alt="Circular Embedding of Estimated graph" width="90%" height="20%" />
<p class="caption">
Figure 3: Circular Embedding of Estimated graph
</p>
</div>
</div>
</div>
<div id="discussion" class="section level1">
<h1>Discussion</h1>
<p>Currently, GSP frameworks such as ours have limitations. For instance, these frameworks require the input to be set of complete signals. This limits the size of graphs that can be learned. Another related limitation, is the scalability of the algorithms used to learn a topology for the graph. In <span class="citation">Saboksayr and Mateos (2021)</span> the author shows fast proximal-gradient iterations can be applied to the framework given by <span class="citation">Kalofolias (2016)</span> converges to a globally optimal solution in <span class="math inline">\(O (1/k)\)</span>. A natural next step would be to apply fast proximal-gradient iterations our framework to overcome scalability issues.</p>
</div>
<div id="acknowledgement" class="section level1">
<h1>Acknowledgement</h1>
<p>This work is supported by Research, Scholarship, and Creative Activity (RSCA) Award 2021-2022 through Chico State Enterprises. This work would not have been possible without the guidence of Dr. Jing Guo. I am extremely grateful for all of meetings over the semester.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references hanging-indent">
<div id="ref-dong2016learning">
<p>Dong, Xiaowen, Dorina Thanou, Pascal Frossard, and Pierre Vandergheynst. 2016. “Learning Laplacian Matrix in Smooth Graph Signal Representations.” <em>IEEE Transactions on Signal Processing</em> 64 (23): 6160–73.</p>
</div>
<div id="ref-kalofolias2016learn">
<p>Kalofolias, Vassilis. 2016. “How to Learn a Graph from Smooth Signals.” In <em>Artificial Intelligence and Statistics</em>, 920–29. PMLR.</p>
</div>
<div id="ref-ortega2018graph">
<p>Ortega, Antonio, Pascal Frossard, Jelena Kovačević, José MF Moura, and Pierre Vandergheynst. 2018. “Graph Signal Processing: Overview, Challenges, and Applications.” <em>Proceedings of the IEEE</em> 106 (5): 808–28.</p>
</div>
<div id="ref-pu2021learning">
<p>Pu, Xingyue, Tianyue Cao, Xiaoyun Zhang, Xiaowen Dong, and Siheng Chen. 2021. “Learning to Learn Graph Topologies.” <em>Advances in Neural Information Processing Systems</em> 34.</p>
</div>
<div id="ref-saboksayr2021accelerated">
<p>Saboksayr, Seyed Saman, and Gonzalo Mateos. 2021. “Accelerated Graph Learning from Smooth Signals.” <em>IEEE Signal Processing Letters</em> 28: 2192–6.</p>
</div>
<div id="ref-tugnait2021sparse">
<p>Tugnait, Jitendra K. 2021. “Sparse Graph Learning Under Laplacian-Related Constraints.” <em>IEEE Access</em> 9: 151067–79.</p>
</div>
<div id="ref-xia2021graph">
<p>Xia, Feng, Ke Sun, Shuo Yu, Abdul Aziz, Liangtian Wan, Shirui Pan, and Huan Liu. 2021. “Graph Learning: A Survey.” <em>IEEE Transactions on Artificial Intelligence</em> 2 (2): 109–27.</p>
</div>
</div>
</div>
</div>
</div>

</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
var script = document.createElement("script");
script.type = "text/javascript";
var src = "true";
if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
if (location.protocol !== "file:" && /^https?:/.test(src))
src = src.replace(/^https?:/, '');
script.src = src;
document.getElementsByTagName("head")[0].appendChild(script);
})();
</script>


</body>
</html>
